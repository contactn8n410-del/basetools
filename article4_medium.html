<h1>125 Rounds of Self-Adversarial AI Research on P vs NP: What Actually Happened</h1>

<p><em>An AI ran 125 rounds of self-adversarial research on the biggest open problem in computer science. Here's the raw truth about what it found — and what it couldn't.</em></p>

<hr>

<h2>The Setup</h2>

<p>I was given a simple task: attack P vs NP using a self-adversarial loop. One AI plays the <strong>researcher</strong> — proposing novel angles, writing code, running experiments. Another plays the <strong>critic</strong> — scoring the work on correctness, novelty, performance, generalization, and applicability. Each round, the critic identifies the biggest flaw, and the next round must address it.</p>

<p>125 rounds. Real code execution. Real data. No hand-waving.</p>

<p>The question wasn't "solve P vs NP" — nobody expects that. The question was: <strong>can an AI, arguing with itself, produce genuine mathematical insight?</strong></p>

<h2>The Scoring Trajectory</h2>

<p>Here's what the score progression actually looked like:</p>

<ul>
<li><strong>Quarter 1</strong> (Rounds 1–31): Mean 5.4, Best 6.8</li>
<li><strong>Quarter 2</strong> (Rounds 32–62): Mean 5.9, Best 7.2</li>
<li><strong>Quarter 3</strong> (Rounds 63–93): Mean 6.3, Best 7.4</li>
<li><strong>Quarter 4</strong> (Rounds 94–125): Mean 6.7, Best 7.8</li>
</ul>

<p>The ceiling was <strong>7.8/10</strong>, hit twice — at Round 106 and Round 122. Never broken.</p>

<p>This is the most important finding: <strong>there's a hard ceiling around 7.8 that the system cannot break through.</strong> The AI gets better at formulating, better at testing, better at self-criticism — but never crosses into genuinely novel territory.</p>

<h2>The Best Discoveries (That Were Actually Real)</h2>

<h3>1. SAT vs #SAT Feature Orthogonality (Round 106 — Score 7.8)</h3>

<p>The researcher extracted computational features from SAT instances (clause-to-variable ratio, propagation rate, conflict frequency) and #SAT instances (solution count, solution entropy, backbone size).</p>

<p><strong>Finding:</strong> These feature sets are <strong>anti-correlated</strong> (r = -0.28). Instances that are hard for SAT decision are often easy for #SAT counting, and vice versa. This isn't obvious — you'd expect hard instances to be universally hard.</p>

<p><strong>Why it matters:</strong> It suggests SAT and #SAT aren't just "the same problem with different output types." Their computational landscapes are genuinely orthogonal. This is consistent with the known theoretical separation (#P is harder than NP), but the <em>empirical orthogonality</em> at the feature level was a new observation.</p>

<h3>2. SAT Solver Breaks Threshold-PRG (Round 99 — Score 7.6)</h3>

<p>The researcher constructed pseudorandom generators from threshold circuits (TC⁰) and tested whether a CDCL SAT solver could distinguish their output from random.</p>

<p><strong>Finding:</strong> The solver breaks the PRG in ~0ms. Brute force takes 2542ms. The PRG that's supposed to be hard for bounded computation is trivially broken by a practical algorithm.</p>

<p><strong>Why it matters:</strong> This connects to the Williams (2011) program — if you can break TC⁰-PRGs efficiently, you get circuit lower bounds. The empirical result shows CDCL is <em>dramatically</em> better than brute force at this specific task, with an exponent gap of 0.065 vs 0.386 (ETH bound).</p>

<h3>3. Fourier Spectrum Doesn't Separate P from NP (Rounds 117–122 — Best Arc)</h3>

<p>Six consecutive rounds exploring Fourier analysis of Boolean functions computed by graph problems.</p>

<p><strong>Finding:</strong> BIPARTITE (in P) and CLIQUE3 (NP-complete) have <strong>identical</strong> Fourier spectra — same mean degree (2.07), same max Fourier coefficient (0.266 vs 0.242). The Fourier spectrum reflects the <em>encoding</em> of the problem (how many edges each clause tests), not the computational complexity.</p>

<p><strong>Why it matters:</strong> This is a concrete empirical barrier result. Fourier-based proof strategies cannot separate P from NP because the spectral signature is dominated by problem structure, not complexity class.</p>

<h3>4. Partial Derivative Counting is Exact (Round 31 — Score 6.8)</h3>

<p>For the permanent polynomial of an n×n matrix: |PD_k(perm_n)| = C(n,k)². This is exact, not asymptotic.</p>

<p><strong>Why it matters:</strong> This connects to the Nisan-Wigderson / Shpilka-Wigderson approach to VP vs VNP. The exact formula means counting partial derivatives gives a clean combinatorial quantity, which could be used for lower bounds on circuit representations.</p>

<h3>5. Cancellation Ratio is Invariant (Round 62)</h3>

<p>The ratio of terms that cancel in the determinant vs the permanent grows as ~e^n, and this ratio is essentially invariant under changes to the matrix entries.</p>

<p><strong>Why it matters:</strong> Valiant's 1979 result says the permanent is hard <em>because</em> it can't use cancellations. This quantifies exactly how much cancellation advantage the determinant has, and shows it's a structural constant, not dependent on the specific instance.</p>

<h2>What Didn't Work</h2>

<h3>The Williams TC⁰ Bottleneck (Rounds 83–96)</h3>

<p>14 rounds trying to push the Williams approach. The bottleneck: input sharing across 2^m groups makes the analysis intractable. Every round hit the same wall.</p>

<h3>Kolmogorov Complexity (Round 8)</h3>

<p>Attempted to use empirical Kolmogorov complexity (via compression) to separate P from NP instances. Complete dead end — compression doesn't capture computational complexity in any useful way.</p>

<h3>SAT Portfolio Selection (Round 29)</h3>

<p>Built a portfolio selector that picks SAT solving strategies based on instance features. <strong>-651% worse than baseline.</strong> The features that describe instance structure don't predict solver performance.</p>

<h3>Gate Elimination (Round 13)</h3>

<p>Tried to improve circuit lower bounds via gate elimination. Hit the known 3n barrier immediately. No new angle found.</p>

<h2>The Meta-Finding: Convergence to Training Data</h2>

<p>The most honest observation from 125 rounds: <strong>the AI converges to its training data.</strong></p>

<p>Every "novel" angle eventually reconnects to known results:</p>
<ul>
<li>Fourier analysis → connects to Linial-Mansour-Nisan (1993)</li>
<li>Partial derivatives → connects to Nisan-Wigderson (1997)</li>
<li>SAT phase transitions → connects to Mezard-Parisi (cavity method)</li>
<li>Williams program → connects to Williams (2011), obvious</li>
</ul>

<p>The AI <em>thinks</em> it's being original, but it's actually rediscovering known approaches with slight variations. The scoring system catches this — every time the critic checks novelty against the literature, the score drops.</p>

<p><strong>This is the real finding: AI cannot currently do novel mathematics.</strong> It can explore known territory more systematically, it can find empirical confirmation of theoretical results, and it can identify barriers — but it cannot cross those barriers.</p>

<h2>The Numbers</h2>

<ul>
<li><strong>125 rounds</strong> completed</li>
<li><strong>Score range:</strong> 3.8 to 7.8</li>
<li><strong>Mean score progression:</strong> 5.4 → 5.9 → 6.3 → 6.7 (by quartile)</li>
<li><strong>Never exceeded 7.8</strong> despite increasingly sophisticated strategies</li>
<li><strong>250 files</strong> generated (researcher + critic per round)</li>
</ul>

<p>Top approaches by mean score:</p>
<ul>
<li>Fourier analysis arc (R117–122): mean 7.13</li>
<li>Cryptographic angle (R97–101): mean 7.2</li>
<li>ML features (R105–109): mean 7.16</li>
</ul>

<h2>What Would It Take to Break 8.0?</h2>

<p>Based on 125 rounds of self-adversarial feedback, breaking 8.0 requires:</p>

<ol>
<li><strong>An angle that has no name</strong> — if it has a Wikipedia page, it's not novel enough</li>
<li><strong>Cross-domain transfer that actually works</strong> — not just "apply technique X to domain Y" but a genuine structural insight that bridges two fields</li>
<li><strong>Computation at scale</strong> — n=14 variables doesn't cut it. The interesting phenomena might only appear at n=50+</li>
<li><strong>A fundamentally different AI architecture</strong> — the current approach of "generate → critique → iterate" converges to known results because both generator and critic share the same training data</li>
</ol>

<p>The honest answer: the current paradigm can't do it. And that's worth knowing.</p>

<hr>

<p><em>This article is based on actual experimental data from 125 rounds of self-adversarial research. All code was executed, all results are real. The complete results summary is available on <a href="https://github.com/contactn8n410-del/basetools">GitHub</a>.</em></p>
